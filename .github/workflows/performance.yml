name: Performance Monitoring

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * SUN'
  workflow_dispatch:
    inputs:
      benchmark_games:
        description: 'Number of benchmark games to run'
        required: false
        default: '1000'
      stress_test:
        description: 'Run stress test'
        type: boolean
        required: false
        default: false

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory-profiler psutil

    - name: Run standard benchmark
      id: benchmark
      run: |
        echo "Starting benchmark with ${{ github.event.inputs.benchmark_games || '1000' }} games"
        python -m wordle_bot.main benchmark \
          --games ${{ github.event.inputs.benchmark_games || '1000' }} \
          --no-display \
          --output benchmark-results.json
        
        # Extract key metrics
        python -c "
import json
with open('benchmark-results.json') as f:
    data = json.load(f)
    print(f'WIN_RATE={data[\"win_rate\"]:.4f}')
    print(f'AVG_GUESSES={data[\"avg_guesses\"]:.2f}')
    print(f'AVG_TIME={data[\"avg_time_per_game\"]:.3f}')
        " >> $GITHUB_OUTPUT

    - name: Run memory profiling
      run: |
        python -m memory_profiler -o memory-profile.txt \
          wordle_bot/main.py benchmark --games 50 --no-display

    - name: Run stress test
      if: github.event.inputs.stress_test == 'true' || github.event_name == 'schedule'
      run: |
        echo "Running stress test..."
        python -m wordle_bot.main benchmark \
          --games 5000 \
          --no-display \
          --output stress-test-results.json

    - name: Generate performance report
      run: |
        python -c "
import json
import os

print('## Performance Report')
print()

if os.path.exists('benchmark-results.json'):
    with open('benchmark-results.json') as f:
        data = json.load(f)
    
    print('### Standard Benchmark Results')
    print(f'- **Win Rate**: {data[\"win_rate\"]:.2%}')
    print(f'- **Average Guesses**: {data[\"avg_guesses\"]:.2f}')
    print(f'- **Average Time**: {data[\"avg_time_per_game\"]:.3f}s')
    print(f'- **Games Played**: {data[\"total_games\"]}')
    print()
    
    print('### Guess Distribution')
    for guess_count, percentage in data['guess_distribution'].items():
        print(f'- {guess_count} guesses: {percentage:.1%}')
    print()

if os.path.exists('stress-test-results.json'):
    with open('stress-test-results.json') as f:
        stress_data = json.load(f)
    
    print('### Stress Test Results')
    print(f'- **Win Rate**: {stress_data[\"win_rate\"]:.2%}')
    print(f'- **Average Guesses**: {stress_data[\"avg_guesses\"]:.2f}')
    print(f'- **Total Games**: {stress_data[\"total_games\"]}')

if os.path.exists('memory-profile.txt'):
    print()
    print('### Memory Usage')
    with open('memory-profile.txt') as f:
        lines = f.readlines()
        max_memory = max([float(line.split()[2]) for line in lines if line.strip() and not line.startswith('Filename')])
        print(f'- **Peak Memory Usage**: {max_memory:.1f} MB')
        " > performance-report.md

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance-report.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });

    - name: Upload performance artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          benchmark-results.json
          stress-test-results.json
          memory-profile.txt
          performance-report.md

    - name: Performance regression check
      if: github.event_name == 'pull_request'
      run: |
        # Simple regression check - fail if win rate drops below 95%
        python -c "
import json
import sys

with open('benchmark-results.json') as f:
    data = json.load(f)

win_rate = data['win_rate']
avg_guesses = data['avg_guesses']

print(f'Current performance: Win Rate {win_rate:.2%}, Avg Guesses {avg_guesses:.2f}')

if win_rate < 0.95:
    print(f'❌ Performance regression detected: Win rate {win_rate:.2%} below 95% threshold')
    sys.exit(1)

if avg_guesses > 4.0:
    print(f'❌ Performance regression detected: Average guesses {avg_guesses:.2f} above 4.0 threshold')
    sys.exit(1)

print('✅ Performance check passed')
        "